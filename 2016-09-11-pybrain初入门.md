---
title: "pybrain初入门"
date: 2016-09-11 00:00:00
category: 科学计算
tags: [机器学习,神经网络]
---

# pybrain初入门

> 标准的官方网址：http://pybrain.org/
> 
> 在python语言中自己实现神经网络的所有代码很复杂，但是有了pybrain就容易的多了，我们只需要专注于算法本身，而忽略算法的繁琐细节


## pybrain的介绍

|   基本流程   |
| :------ |
|  1.构造神经网络  |
|  2.构造数据集  |
|  3.训练神经网络  |
|  4.结果可视化  |
|  5.验证与分析 |

## pybrain使用入门

> 参考文献: 
> 
> 下面的基本用法将逐步的完善补充，主要还是根据自己的学习进度进行推进

```python
#coding=utf-8
import numpy as np

def generate_data():
    """generate original data of u and y"""
    u = np.random.uniform(-1,1,200)
    y=[]
    former_y_value = 0
    for i in np.arange(0,200):
        y.append(former_y_value)
        next_y_value = (29 / 40) * np.sin(
            (16 * u[i] + 8 * former_y_value) / (3 + 4 * (u[i] ** 2) + 4 * (former_y_value ** 2))) \
                       + (2 / 10) * u[i] + (2 / 10) * former_y_value
        former_y_value = next_y_value
    return u,y
#大概分为以下这几步。

# 构造神经网络
# 构造数据集
# 训练神经网络
# 结果可视化
# 验证与分析
#1.构造神经网络
from pybrain.structure import *
# 建立神经网络fnn
fnn = FeedForwardNetwork()

# 设立三层，一层输入层（3个神经元，别名为inLayer），一层隐藏层，一层输出层
inLayer = LinearLayer(2, name='inLayer')
hiddenLayer = SigmoidLayer(10, name='hiddenLayer0')
outLayer = LinearLayer(1, name='outLayer')

# 将三层都加入神经网络（即加入神经元）
fnn.addInputModule(inLayer)
fnn.addModule(hiddenLayer)
fnn.addOutputModule(outLayer)

# 建立三层之间的连接
in_to_hidden = FullConnection(inLayer, hiddenLayer)
hidden_to_out = FullConnection(hiddenLayer, outLayer)

# 将连接加入神经网络
fnn.addConnection(in_to_hidden)
fnn.addConnection(hidden_to_out)

# 让神经网络可用
fnn.sortModules()

#2.构造数据集
#在构造数据集的时候，我用的是SupervisedDataset，即监督数据集。也可以试一试别的。
from pybrain.supervised.trainers import BackpropTrainer
from pybrain.datasets import SupervisedDataSet

# 定义数据集的格式是三维输入，一维输出
DS = SupervisedDataSet(2,1)

# 往数据集内加样本点
# 假设x1，x2，x3是输入的三个维度向量，y是输出向量，并且它们的长度相同
u,y = generate_data()
for i in np.arange(199):
    DS.addSample([u[i], y[i]], [y[i+1]])

# 如果要获得里面的输入／输出时，可以用
X = DS['input']
Y = DS['target']
# print(X,Y)
# 如果要把数据集切分成训练集和测试集，可以用下面的语句，训练集：测试集＝8:2
# 为了方便之后的调用，可以把输入和输出拎出来
dataTrain, dataTest = DS.splitWithProportion(0.8)
xTrain, yTrain = dataTrain['input'], dataTrain['target']
xTest, yTest = dataTest['input'], dataTest['target']
# print dataTest

# 3.训练神经网络
#俗话说得好，80%的工作往往是20%的部分完成的。嗯哼，其实最重要的代码就是如下这几行啦。
# 不过调用的是别人的东西，也不知道内部的实现比例，就是开个玩笑。
from pybrain.supervised.trainers import BackpropTrainer

# 训练器采用BP算法
# verbose = True即训练时会把Total error打印出来，库里默认训练集和验证集的比例为4:1，可以在括号里更改
trainer = BackpropTrainer(fnn, dataTrain, verbose = False, learningrate=0.01)

# maxEpochs即你需要的最大收敛迭代次数，这里采用的方法是训练至收敛，我一般设为1000
trainer.trainUntilConvergence(maxEpochs=1000)

#4.结果可视化
# 数据可视化就不提了，基本上用的是Pylab来进行数据可视化


#5.验证与分析
import matplotlib.pyplot as plt
predict_resutl=[]
for i in np.arange(len(xTest)):
    predict_resutl.append(fnn.activate(xTest[i])[0])
print(predict_resutl)

plt.figure()
plt.plot(np.arange(0,len(xTest)), predict_resutl, 'ro--', label='predict number')
plt.plot(np.arange(0,len(xTest)), yTest, 'ko-', label='true number')
plt.legend()
plt.xlabel("x")
plt.ylabel("y")

plt.show()

for mod in fnn.modules:
  print "Module:", mod.name
  if mod.paramdim > 0:
    print "--parameters:", mod.params
  for conn in fnn.connections[mod]:
    print "-connection to", conn.outmod.name
    if conn.paramdim > 0:
       print "- parameters", conn.params
  if hasattr(fnn, "recurrentConns"):
    print "Recurrent connections"
    for conn in fnn.recurrentConns:             
       print "-", conn.inmod.name, " to", conn.outmod.name
       if conn.paramdim > 0:
          print "- parameters", conn.params
```

## 机器学习相关的库

> 
> 1. scikit_learn   机器学习方面的库
> 2. pandas         处理数据文本用
> 3. tensorflow     谷歌的深度学习，神经网络库
> 4. pybrain        机器学习中神经网络的库
> 5. NLTK           自然语言处理
> 6. xgboost        预测模型


# 参考：
[1]: http://pybrain.org/